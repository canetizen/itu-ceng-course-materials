{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mustafa Can Caliskan, 150200097"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMYA2W-BDbaK",
        "outputId": "0fa7536e-2017-4319-82d7-a1885a83d2a9"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Create a DataFrame from the dataset\n",
        "iris_df = pd.DataFrame(data=np.c_[iris['data'], iris['target']], columns=iris['feature_names'] + ['target'])\n",
        "\n",
        "# Save the dataset to a CSV file\n",
        "iris_df.to_csv('iris_dataset.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eBEZ6AtDbaM",
        "outputId": "6ddc5f29-db8a-4bfb-a2da-bee9ed1a2f72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The proportion of true cases is: 1.0\n"
          ]
        }
      ],
      "source": [
        "class Node:\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
        "        # Initializes properties for a decision tree node\n",
        "        self.feature = feature  # Feature used for node splitting\n",
        "        self.threshold = threshold  # Threshold for node splitting\n",
        "        self.left = left  # Left child node\n",
        "        self.right = right  # Right child node\n",
        "        self.value = value  # Class label stored in leaf node\n",
        "\n",
        "    def isLeafNode(self):\n",
        "        # Checks if the node is a leaf node\n",
        "        return self.value is not None\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, minSamplesSplit=2, maxDepth=100, nFeatures=None):\n",
        "        # Initializes parameters for the decision tree model\n",
        "        self.minSamplesSplit = minSamplesSplit  # Minimum samples required to split a node\n",
        "        self.maxDepth = maxDepth  # Maximum depth of the tree\n",
        "        self.nFeatures = nFeatures  # Number of features to consider for splits\n",
        "        self.root = None  # Initializing the root node of the tree\n",
        "\n",
        "    def train(self, X, y):\n",
        "        # Trains the decision tree on the given dataset\n",
        "        self.nFeatures = X.shape[1] if not self.nFeatures else min(X.shape[1], self.nFeatures)\n",
        "        self.root = self.growTree(X, y)\n",
        "\n",
        "    def growTree(self, X, y, depth=0):\n",
        "        # Recursively grows the decision tree\n",
        "        nSamples, nFeats = X.shape\n",
        "        nLabels = len(np.unique(y))\n",
        "        # Termination conditions for recursion or creating a leaf node\n",
        "        if (depth >= self.maxDepth or nLabels == 1 or nSamples < self.minSamplesSplit):\n",
        "            leafValue = self.mostCommon(y)\n",
        "            return Node(value=leafValue)\n",
        "        featIdxs = np.random.choice(nFeats, self.nFeatures, replace=False)\n",
        "        bestFeat, bestThreshold = self.bestSplit(X, y, featIdxs)\n",
        "        leftIdxs, rightIdxs = self.split(X[:, bestFeat], bestThreshold)\n",
        "        left = self.growTree(X[leftIdxs, :], y[leftIdxs], depth + 1)\n",
        "        right = self.growTree(X[rightIdxs, :], y[rightIdxs], depth + 1)\n",
        "        return Node(bestFeat, bestThreshold, left, right)\n",
        "\n",
        "    def mostCommon(self, y):\n",
        "        # Finds the most common label in the given set of labels\n",
        "        y = sorted(y)\n",
        "        frequencies = {}\n",
        "        for element in y:\n",
        "            if element in frequencies:\n",
        "                frequencies[element] += 1\n",
        "            else:\n",
        "                frequencies.update({element: 1})\n",
        "        return max(frequencies, key=frequencies.get)\n",
        "\n",
        "    def bestSplit(self, X, y, featIdxs):\n",
        "        # Finds the best feature and threshold for splitting based on information gain\n",
        "        bestGain = -1\n",
        "        splitIdx, splitThreshold = None, None\n",
        "        for featIdx in featIdxs:\n",
        "            XColumn = X[:, featIdx]\n",
        "            thresholds = np.unique(XColumn)\n",
        "            for thr in thresholds:\n",
        "                gain = self.informationGain(y, XColumn, thr)\n",
        "                if gain > bestGain:\n",
        "                    bestGain = gain\n",
        "                    splitIdx = featIdx\n",
        "                    splitThreshold = thr\n",
        "        return splitIdx, splitThreshold\n",
        "\n",
        "    def informationGain(self, y, XColumn, threshold):\n",
        "        # Calculates information gain based on a split\n",
        "        parentEntropy = self.entropy(y)\n",
        "        leftIdxs, rightIdxs = self.split(XColumn, threshold)\n",
        "        if len(leftIdxs) == 0 or len(rightIdxs) == 0:\n",
        "            return 0\n",
        "        n = len(y)\n",
        "        nL, nR = len(leftIdxs), len(rightIdxs)\n",
        "        eL, eR = self.entropy(y[leftIdxs]), self.entropy(y[rightIdxs])\n",
        "        childEntropy = (nL / n) * eL + (nR / n) * eR\n",
        "        informationGain = parentEntropy - childEntropy\n",
        "        return informationGain\n",
        "\n",
        "    def split(self, XColumn, splitThresh):\n",
        "        # Splits the dataset into left and right indices based on a threshold\n",
        "        leftIdxs = np.argwhere(XColumn <= splitThresh).flatten()\n",
        "        rightIdxs = np.argwhere(XColumn > splitThresh).flatten()\n",
        "        return leftIdxs, rightIdxs\n",
        "\n",
        "    def entropy(self, y):\n",
        "        # Calculates the entropy of a set of labels\n",
        "        uniqw, inverse = np.unique(y, return_inverse=True)\n",
        "        hist = np.bincount(inverse)\n",
        "        ps = hist / len(y)\n",
        "        return -np.sum([(p * np.log(p + 1e-5) + (1 - p) * np.log(1 - p + 1e-5)) for p in ps if p > 0])\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Predicts the labels for input samples\n",
        "        return np.array([self.traverseTree(x, self.root) for x in X])\n",
        "\n",
        "    def traverseTree(self, x, node):\n",
        "        # Traverses the tree to predict a label for a single sample\n",
        "        if node.isLeafNode():\n",
        "            return node.value\n",
        "\n",
        "        if x[node.feature] <= node.threshold:\n",
        "            return self.traverseTree(x, node.left)\n",
        "        return self.traverseTree(x, node.right)\n",
        "\n",
        "\n",
        "data = pd.read_csv('iris_dataset.csv')\n",
        "X = data.drop('target', axis=1)\n",
        "y = data['target']\n",
        "X = X.values\n",
        "y = y.values\n",
        "dt = DecisionTree()\n",
        "dt.train(X, y)\n",
        "predictions = dt.predict(X)\n",
        "output_df = pd.DataFrame({'Actual': y, 'Predicted': predictions})\n",
        "output_df.to_csv('decision_tree_predictions.csv', index=False)\n",
        "df = pd.read_csv('decision_tree_predictions.csv')\n",
        "trueProportion = (df['Actual'] == df['Predicted']).mean()\n",
        "print(f\"The proportion of true cases is: {trueProportion}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "b5juPyTYDbaN"
      },
      "outputs": [],
      "source": [
        "class Autoencoder:\n",
        "    def __init__(self, input_dim, hidden_dim, learning_rate=0.01, epochs=10000):\n",
        "        # Initialize the autoencoder with input dimensions, hidden layer dimensions, learning rate, and epochs\n",
        "        self.inputDim = input_dim\n",
        "        self.hiddenDim = hidden_dim\n",
        "        self.outputDim = input_dim\n",
        "        self.learningRate = learning_rate\n",
        "        self.epochs = epochs\n",
        "\n",
        "        # Initialize weights randomly for the encoder and decoder layers\n",
        "        self.W1 = np.random.normal(size=(input_dim, hidden_dim))\n",
        "        self.W2 = np.random.normal(size=(hidden_dim, self.outputDim))\n",
        "\n",
        "    def tanh(self, x):\n",
        "        # Define the hyperbolic tangent activation function\n",
        "        return np.tanh(x)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Perform the forward pass through the autoencoder\n",
        "        # Calculate the output of the hidden layer using tanh activation\n",
        "        hidden = self.tanh(np.dot(X, self.W1))\n",
        "        # Calculate the output of the decoder layer\n",
        "        output = np.dot(hidden, self.W2)\n",
        "        return hidden, output\n",
        "\n",
        "    def backward(self, X, hidden, output):\n",
        "        # Perform backpropagation to update weights\n",
        "        # Calculate gradients for the weights of the decoder layer\n",
        "        dW2 = np.dot(hidden.T, (output - X))\n",
        "        # Calculate gradients for the weights of the encoder layer\n",
        "        dW1 = np.dot(X.T, np.dot((output - X), self.W2.T) * (1 - hidden**2))\n",
        "        return dW1, dW2\n",
        "\n",
        "    def normalize(self, X):\n",
        "        # Normalize the input data\n",
        "        # Find min and max values of each column for later denormalization\n",
        "        self.originalMin = np.min(X, axis=0)\n",
        "        self.originalMax = np.max(X, axis=0)\n",
        "        return (X - self.originalMin) / (self.originalMax - self.originalMin)\n",
        "\n",
        "    def denormalize(self, XNormalized):\n",
        "        # Denormalize the data back to its original scale\n",
        "        originalMin = np.reshape(self.originalMin, (1, -1))\n",
        "        originalMax = np.reshape(self.originalMax, (1, -1))\n",
        "        return XNormalized * (originalMax - originalMin) + originalMin\n",
        "\n",
        "    def encode(self, X):\n",
        "        # Encodes the input data by passing it through the encoder layer\n",
        "        return self.tanh(np.dot(X, self.W1))\n",
        "\n",
        "    def decode(self, encodedData):\n",
        "        # Decodes the encoded data by passing it through the decoder layer\n",
        "        return np.dot(encodedData, self.W2)\n",
        "\n",
        "    def train(self, X):\n",
        "        # Training the autoencoder using the input data\n",
        "        for epoch in range(self.epochs):\n",
        "            # Forward pass\n",
        "            hidden, output = self.forward(X)\n",
        "            # Backward pass to calculate gradients\n",
        "            dW1, dW2 = self.backward(X, hidden, output)\n",
        "\n",
        "            # Update weights using gradients and learning rate\n",
        "            self.W1 -= self.learningRate * dW1\n",
        "            self.W2 -= self.learningRate * dW2\n",
        "\n",
        "\n",
        "data = pd.read_csv('iris_dataset.csv')\n",
        "X = data.drop('target', axis=1)\n",
        "input_size = X.shape[1]\n",
        "hidden_size = 2\n",
        "ae = Autoencoder(input_size, hidden_size)\n",
        "normalized_X = ae.normalize(X)\n",
        "ae.train(normalized_X)\n",
        "encoded_data = ae.encode(normalized_X)\n",
        "decoded_data = ae.decode(encoded_data)\n",
        "decoded_data = decoded_data = ae.denormalize(decoded_data)\n",
        "\n",
        "output_df = pd.DataFrame(decoded_data, columns=X.columns)\n",
        "output_df.to_csv('autoencoder_output.csv', index=False)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
